{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yolo.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pA6pcKcvmMBd"},"source":["# Welcome to the YOLO Workshop!\n","\n","In this workshop, we will learn how to use the YOLO algorithm for object detection"]},{"cell_type":"markdown","metadata":{"id":"jOA6AThYjTc3"},"source":["## Import the necessary libraries\n","We will need OpenCV, Matplotlib, and NumPy"]},{"cell_type":"code","metadata":{"id":"C3Zq-cGRa4KR","executionInfo":{"status":"ok","timestamp":1601151068243,"user_tz":-210,"elapsed":23766,"user":{"displayName":"Mouse Routing","photoUrl":"","userId":"13348306626310405776"}},"outputId":"d0195c92-e8fc-47f9-b4d3-23d7b5fd4aed","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZCskEUmRdUhG"},"source":["import cv2 as cv\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQ0MBU9GjZ8s"},"source":["## Define the class YOLO and the init() function"]},{"cell_type":"code","metadata":{"id":"UkwZfOGYmZoW"},"source":["class YOLO():\n","    def __init__(self):\n","        \"\"\"\n","        - YOLO takes an image as input. We should set the dimension of the image to a fixed number.\n","        - The default choice is often 416x416.\n","        - YOLO applies thresholding and non maxima suppression, define a value for both\n","        - Load the classes, model configuration (cfg file) and pretrained weights (weights file) into variables\n","        - If the image is 416x416, the weights must be corresponding to that image\n","        - Load the network with OpenCV.dnn function\n","        \"\"\"\n","        self.confThreshold = 0.5\n","        self.nmsThreshold = 0.4\n","        self.inpWidth = 480\n","        self.inpHeight = 480\n","        classesFile = \"/content/drive/My Drive/Step-3 - Detection- tracking/YOLO/obj.names\"\n","        self.classes = None\n","        with open(classesFile,'rt') as f:\n","            self.classes = f.read().rstrip('\\n').split('\\n')\n","        modelConfiguration = \"/content/drive/My Drive/Step-3 - Detection- tracking/YOLO/yolov3.cfg\";\n","        modelWeights = \"/content/drive/My Drive/Step-3 - Detection- tracking/YOLO/yolov3_last3.weights\";\n","        self.i = 0\n","        self.net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n","        self.net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n","        self.net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a35sfY7pizEO"},"source":["def getOutputsNames(self):\n","    '''\n","    Get the names of the output layers\n","    '''\n","    # Get the names of all the layers in the network\n","    layersNames = self.net.getLayerNames()\n","    # Get the names of the output layers, i.e. the layers with unconnected outputs\n","    return [layersNames[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n","\n","YOLO.getOutputsNames = getOutputsNames"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyZqkPYfityC"},"source":["def drawPred(self, frame, classId, conf, left, top, right, bottom):\n","    '''\n","    Draw a bounding box around a detected object given the box coordinates\n","    Later, we could repurpose that to display an ID\n","    '''\n","    # Draw a bounding box.\n","    cv.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), thickness=2)\n","    label = '%.2f' % conf\n","    # Get the label for the class name and its confidence\n","    if self.classes:\n","        assert(classId < len(self.classes))\n","        label = '%s:%s' % (self.classes[classId], label)\n","\n","    #Display the label at the top of the bounding box\n","    labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n","    top = max(top, labelSize[1])\n","    cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), thickness=2)\n","    return frame\n","    \n","YOLO.drawPred = drawPred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"noSq1mv0iooF"},"source":["def postprocess(self, frame, outs):\n","    \"\"\"\n","    Postprocessing step. Take the output out of the neural network and interpret it.\n","    We should use that output to apply NMS thresholding and confidence thresholding\n","    We should use the output to draw the bounding boxes using the dramPred function\n","    \"\"\"\n","    frameHeight = frame.shape[0]\n","    frameWidth = frame.shape[1]\n","    classIds = []\n","    confidences = []\n","    boxes = []\n","    # Scan through all the bounding boxes output from the network and keep only the\n","    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n","    classIds = []\n","    confidences = []\n","    boxes = []\n","    for out in outs:\n","        for detection in out:\n","            scores = detection[5:]\n","            classId = np.argmax(scores)\n","            confidence = scores[classId]\n","            if confidence > self.confThreshold:\n","                center_x = int(detection[0] * frameWidth)\n","                center_y = int(detection[1] * frameHeight)\n","                width = int(detection[2] * frameWidth)\n","                height = int(detection[3] * frameHeight)\n","                left = int(center_x - width / 2)\n","                top = int(center_y - height / 2)\n","                classIds.append(classId)\n","                confidences.append(float(confidence))\n","                boxes.append([left, top, width, height])\n","    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n","    # lower confidences.\n","    indices = cv.dnn.NMSBoxes(boxes, confidences, self.confThreshold, self.nmsThreshold)\n","    \n","    output_image = frame\n","    \n","    for i in indices:\n","        i = i[0]\n","        box = boxes[i]\n","        left = box[0]\n","        top = box[1]\n","        width = box[2]\n","        height = box[3]\n","        output_image = self.drawPred(frame,classIds[i], confidences[i], left, top, left + width, top + height)\n","    return output_image\n","\n","YOLO.postprocess = postprocess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaDHNd_-iPe-"},"source":["def inference(self,image):\n","    \"\"\"\n","    Main loop.\n","    Input: Image\n","    Output: Frame with the drawn bounding boxes\n","    \"\"\"\n","    # Create a 4D blob from a frame.\n","    blob = cv.dnn.blobFromImage(image, 1/255, (self.inpWidth, self.inpHeight), [0,0,0], 1, crop=False)\n","    # Sets the input to the network\n","    self.net.setInput(blob)\n","    # Runs the forward pass to get output of the output layers\n","    outs = self.net.forward(self.getOutputsNames())\n","    # Remove the bounding boxes with low confidence\n","    final_frame = self.postprocess(image, outs)\n","    self.i +=1\n","    return final_frame\n","\n","YOLO.inference = inference"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hR3hOL0toWhw","executionInfo":{"status":"ok","timestamp":1601151178652,"user_tz":-210,"elapsed":4629,"user":{"displayName":"Mouse Routing","photoUrl":"","userId":"13348306626310405776"}},"outputId":"d5abc05b-8c87-4393-d18f-026e0b7e0b22","colab":{"base_uri":"https://localhost:8080/","height":268}},"source":["img = mpimg.imread(\"/content/drive/My Drive/Step-3 - Detection- tracking/YOLO/test_images/Validation_80.png\")\n","yolo = YOLO()\n","oi = yolo.inference(img)\n","plt.imshow(oi)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"e_LKKEgINKYt"},"source":["### Detection on a video!\n","\n","A video is just a set of frames, we will call the inference function for each frame of the video and save it.\n"]},{"cell_type":"code","metadata":{"id":"iX4EDZiZNU7p","executionInfo":{"status":"ok","timestamp":1601152806021,"user_tz":-210,"elapsed":1613134,"user":{"displayName":"Mouse Routing","photoUrl":"","userId":"13348306626310405776"}},"outputId":"09f4ae8d-72d9-47b3-f0d2-cd3272d9e2f1","colab":{"base_uri":"https://localhost:8080/","height":247}},"source":["from moviepy.editor import VideoFileClip\n","video_file = \"/content/drive/My Drive/Step-3 - Detection- tracking/YOLO/test_videos/input.mp4\"\n","clip = VideoFileClip(video_file)\n","white_clip = clip.fl_image(yolo.inference)\n","%time white_clip.write_videofile(\"movie.mp4\",audio=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n","Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2375680/45929032 bytes (5.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5718016/45929032 bytes (12.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9175040/45929032 bytes (20.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12615680/45929032 bytes (27.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15998976/45929032 bytes (34.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19472384/45929032 bytes (42.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22855680/45929032 bytes (49.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26222592/45929032 bytes (57.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29466624/45929032 bytes (64.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32931840/45929032 bytes (71.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36175872/45929032 bytes (78.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39378944/45929032 bytes (85.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42631168/45929032 bytes (92.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n","[MoviePy] >>>> Building video movie.mp4\n","[MoviePy] Writing video movie.mp4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 626/626 [26:45<00:00,  2.57s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["[MoviePy] Done.\n","[MoviePy] >>>> Video ready: movie.mp4 \n","\n","CPU times: user 51min 9s, sys: 4.53 s, total: 51min 14s\n","Wall time: 26min 46s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ESj1-vsRQWE0","executionInfo":{"status":"ok","timestamp":1601152829149,"user_tz":-210,"elapsed":3178,"user":{"displayName":"Mouse Routing","photoUrl":"","userId":"13348306626310405776"}},"outputId":"6fcc2b95-1059-42f3-ea49-9a185663b788","colab":{"base_uri":"https://localhost:8080/","height":261,"output_embedded_package_id":"1IEBmhlvd-eDKWiZM-5Zp9RDW3xoCOXoC"}},"source":["import io\n","import base64\n","from IPython.display import HTML\n","\n","video = io.open('movie.mp4', 'r+b').read()\n","encoded = base64.b64encode(video)\n","HTML(data='''<video alt=\"test\" controls width=\"320\" height=\"240\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))) "],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}